{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Convect AI Decision API is a collection of out-of-box APIs to support the most common decision problems in Supply Chain Optimization space. The goal is to let Convect manage the heavy lifting of building and tuning algorithms, allocating computing resources for your decision problems, so you can quickly add intelligence when developing supply chain decision applications. Specifically, Convect currently provides APIs to solve two types of problems: Automated forecasting Supply-demand planning Quickstart Convect AI implements the REST style APIs to enable users to build and trigger model runs for decision problems programmatically. For a more comprehensive reference on the available APIs, please see reference . Obtaining an API credentials Register on Convect Platform , go to the account settings page to obtain a pair of API id and secrets . Or contact Convect support to obtain a demo key. Once you have your client_id and client_secret , before calling the API, obtain an access token by sending a POST request curl --request POST \\ --url https://convect-dev.us.auth0.com/oauth/token \\ --header 'content-type: application/json' \\ --data '{\"client_id\":\"{YOUR_CLIENT_ID}\",\"client_secret\":\"{YOUR_CLIENT_SECRET}\",\"audience\":\"https://forecast.convect.ai\",\"grant_type\":\"client_credentials\"}' and get a response { \"access_token\" : \"{RETURNED_TOKEN}\" , \"token_type\" : \"Bearer\" } Then you can attach the token in the following API calls. Calling the API Attach the returned token in every header of the requests you send to a Convect API. For example, set Authorization: Bearer {RETURNED_TOKEN} in the header. Convect APIs also accept and return json type data, so you can set Accept: application/json and Content-type: application/json in your header as well. Example - Automated forecasting Here is an example of training and generating forecast by calling the automated forecasting APIs. python #!/bin/env python import requests import os.path import pandas as pd BASE_ENDPOINT = \"https://forecast.convect.ai/api/\" DEMO_KEY = \"YOUR_API_KEY\" HEADER = { \"Accept\" : \"application/json\" , \"Content-type\" : \"application/json\" , \"Authorization\" : \"Bearer: {} \" . format ( DEMO_KEY ), } # create the data group payload = { \"name\" : \"Demo data group\" } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"data_groups/\" ), json = payload , ) data_group_id = resp . json ()[ \"id\" ] # upload the dataset data_url = \"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\" payload = { \"name\" : \"target time series\" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : data_url , \"file_format\" : \"csv\" , \"frequency\" : \"W\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"sku\" , \"col_type\" : \"key\" }, { \"name\" : \"week\" , \"col_type\" : \"time\" }, { \"name\" : \"qty\" , \"col_type\" : \"num\" }, ], } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"datasets/\" ), json = payload , ) dataset_id = resp . json ()[ \"id\" ] # build the forecating config url = os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ) output_path = \"s3://convect-data/result/demo-run\" payload = { \"name\" : \"14 week forecast config\" , \"result_uri\" : output_path , \"horizon\" : 14 , \"frequency\" : \"W\" , \"data_group\" : data_group_id , } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ), json = payload , ) config_id = resp . json ()[ \"id\" ] # trigger the forecasting payload = { \"predictor_config\" : config_id } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictors/\" ), json = payload ) predictor_id = resp . json ()[ \"id\" ] # query the result resp = requests . get ( os . path . join ( BASE_ENDPOINT , \"predictors/\" , f \" { predictor_id } /\" ) ) print ( resp . json ()) # retrieve the result df = pd . read_csv ( output_path ) curl #!/bin/env bash export BASE_URL = 'https://forecast.convect.ai/api/' export TOKEN = 'DEMO KEY' export AUTH_HEADER = 'Authorization: Bearer ${TOKEN}' # create a dataset group data_group_id = $( curl --request POST $BASE_URL /data_groups/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"name\": \"Demo data group\"}' | jq '.id' ) # upload a dataaset curl --request POST $BASE_URL /datasets/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data-binary @- << EOF { \"name\": \"target time series\", \"dataset_type\": \"TARGET_TIME_SERIES\", \"path\": data_url, \"file_format\": \"csv\", \"frequency\": \"W\", \"data_group\": ${data_group_id}, \"schemas\": [ {\"name\": \"sku\", \"col_type\": \"key\"}, {\"name\": \"week\", \"col_type\": \"time\"}, {\"name\": \"qty\", \"col_type\": \"num\"}, ], } EOF # set up a forecat config output_path = 's3://convect-data/result/demo-run' config_id = $( curl --request POST $BASE_URL /predictor-configs/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data-binary @- << EOF { \"name\": \"12 week forecast config\", \"result_uri\": output_path, \"horizon\": 14, \"frequency\": \"W\", \"data_group\": data_group_id, } EOF | jq '.id' ) # trigger a forecast run run_id = $( curl --request POST $BASE_URL /predictors/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"predictor_config\": \"${config_id}\"}' | jq '.id' ) # query the run status curl --request GET $BASE_URL /predictors/ ${ run_id } / \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER # retrieve the result aws s3 cp $output_path ./result.csv","title":"Overview"},{"location":"#introduction","text":"Convect AI Decision API is a collection of out-of-box APIs to support the most common decision problems in Supply Chain Optimization space. The goal is to let Convect manage the heavy lifting of building and tuning algorithms, allocating computing resources for your decision problems, so you can quickly add intelligence when developing supply chain decision applications. Specifically, Convect currently provides APIs to solve two types of problems: Automated forecasting Supply-demand planning","title":"Introduction"},{"location":"#quickstart","text":"Convect AI implements the REST style APIs to enable users to build and trigger model runs for decision problems programmatically. For a more comprehensive reference on the available APIs, please see reference .","title":"Quickstart"},{"location":"#obtaining-an-api-credentials","text":"Register on Convect Platform , go to the account settings page to obtain a pair of API id and secrets . Or contact Convect support to obtain a demo key. Once you have your client_id and client_secret , before calling the API, obtain an access token by sending a POST request curl --request POST \\ --url https://convect-dev.us.auth0.com/oauth/token \\ --header 'content-type: application/json' \\ --data '{\"client_id\":\"{YOUR_CLIENT_ID}\",\"client_secret\":\"{YOUR_CLIENT_SECRET}\",\"audience\":\"https://forecast.convect.ai\",\"grant_type\":\"client_credentials\"}' and get a response { \"access_token\" : \"{RETURNED_TOKEN}\" , \"token_type\" : \"Bearer\" } Then you can attach the token in the following API calls.","title":"Obtaining an API credentials"},{"location":"#calling-the-api","text":"Attach the returned token in every header of the requests you send to a Convect API. For example, set Authorization: Bearer {RETURNED_TOKEN} in the header. Convect APIs also accept and return json type data, so you can set Accept: application/json and Content-type: application/json in your header as well.","title":"Calling the API"},{"location":"#example-automated-forecasting","text":"Here is an example of training and generating forecast by calling the automated forecasting APIs. python #!/bin/env python import requests import os.path import pandas as pd BASE_ENDPOINT = \"https://forecast.convect.ai/api/\" DEMO_KEY = \"YOUR_API_KEY\" HEADER = { \"Accept\" : \"application/json\" , \"Content-type\" : \"application/json\" , \"Authorization\" : \"Bearer: {} \" . format ( DEMO_KEY ), } # create the data group payload = { \"name\" : \"Demo data group\" } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"data_groups/\" ), json = payload , ) data_group_id = resp . json ()[ \"id\" ] # upload the dataset data_url = \"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\" payload = { \"name\" : \"target time series\" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : data_url , \"file_format\" : \"csv\" , \"frequency\" : \"W\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"sku\" , \"col_type\" : \"key\" }, { \"name\" : \"week\" , \"col_type\" : \"time\" }, { \"name\" : \"qty\" , \"col_type\" : \"num\" }, ], } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"datasets/\" ), json = payload , ) dataset_id = resp . json ()[ \"id\" ] # build the forecating config url = os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ) output_path = \"s3://convect-data/result/demo-run\" payload = { \"name\" : \"14 week forecast config\" , \"result_uri\" : output_path , \"horizon\" : 14 , \"frequency\" : \"W\" , \"data_group\" : data_group_id , } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ), json = payload , ) config_id = resp . json ()[ \"id\" ] # trigger the forecasting payload = { \"predictor_config\" : config_id } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictors/\" ), json = payload ) predictor_id = resp . json ()[ \"id\" ] # query the result resp = requests . get ( os . path . join ( BASE_ENDPOINT , \"predictors/\" , f \" { predictor_id } /\" ) ) print ( resp . json ()) # retrieve the result df = pd . read_csv ( output_path ) curl #!/bin/env bash export BASE_URL = 'https://forecast.convect.ai/api/' export TOKEN = 'DEMO KEY' export AUTH_HEADER = 'Authorization: Bearer ${TOKEN}' # create a dataset group data_group_id = $( curl --request POST $BASE_URL /data_groups/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"name\": \"Demo data group\"}' | jq '.id' ) # upload a dataaset curl --request POST $BASE_URL /datasets/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data-binary @- << EOF { \"name\": \"target time series\", \"dataset_type\": \"TARGET_TIME_SERIES\", \"path\": data_url, \"file_format\": \"csv\", \"frequency\": \"W\", \"data_group\": ${data_group_id}, \"schemas\": [ {\"name\": \"sku\", \"col_type\": \"key\"}, {\"name\": \"week\", \"col_type\": \"time\"}, {\"name\": \"qty\", \"col_type\": \"num\"}, ], } EOF # set up a forecat config output_path = 's3://convect-data/result/demo-run' config_id = $( curl --request POST $BASE_URL /predictor-configs/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data-binary @- << EOF { \"name\": \"12 week forecast config\", \"result_uri\": output_path, \"horizon\": 14, \"frequency\": \"W\", \"data_group\": data_group_id, } EOF | jq '.id' ) # trigger a forecast run run_id = $( curl --request POST $BASE_URL /predictors/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"predictor_config\": \"${config_id}\"}' | jq '.id' ) # query the run status curl --request GET $BASE_URL /predictors/ ${ run_id } / \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER # retrieve the result aws s3 cp $output_path ./result.csv","title":"Example - Automated forecasting"},{"location":"flowopt/data-prepare/","text":"Data preparation WIP","title":"Prepare the data"},{"location":"flowopt/data-prepare/#data-preparation","text":"WIP","title":"Data preparation"},{"location":"flowopt/overview/","text":"","title":"Overview"},{"location":"forecast/data-prepare/","text":"Dataset types supported Convect Forecast supports the following types of datasets: Target time series Related time series Item meta information Target time series data is the only required dataset type need to be provided to build a forecasting model. It records the time series values for an entity at different history timestamps. For example, in a retailing setting, it captures the unit sold for each product at different history dates. Related time series data captures time varying values that are not direct targets to be forecasted associated with entities. For example, it may record the price of a product on each date. Item meta information provides additional information that is static across time about the entity to be forecasted. For example, it may contain categorical information such as brands, categories and vendors, about entities. Data formats All 3 types of datasets require one or multiple columns that indicate the identifiers about entities. For example, in the retail setting, these columns may include a SKU plus a store id that uniquely define the identity of an entity to be forecasted. Target time series The dataset requires at least 3 columns: 1 or more columns serve as the key column(s) mentioned above 1 timestamp column marking when the observation of the target time series value was recorded 1 value column storing the actual value of the time series For example model week qty 1163704 2017-01-01 267 1163704 2017-01-08 229 5998369 2017-01-01 1689 5998369 2017-01-08 1322 where model is the key column of the entity; week is the timestamp column; qty stores the actual time series value. Related time series Similarly to target time series data, the dataset requires at least 3 columns: 1 or more columns serve as the key column(s) mentioned above 1 timestamp column marking when the observation of the target time series value was recorded 1 or more value columns storing the actual values of the related time series values For example model week price temperature 1163704 2017-01-01 12.5 20.5 1163704 2017-01-08 12.3 22.3 5998369 2017-01-01 5.6 20.5 5998369 2017-01-08 4.5 22.3 where model is the key column of the entity; week is the timestamp column; price and temperature store two related time series values about the entity. Item meta information Item meta information requires at least 2 columns: 1 or more columns serve as the key column(s) mentioned above 1 or more value columns storing some meta information about the entity For example model brand category 1163704 XXX Food 5998369 YYY Pet where model is the key column of the entity; brand and category are the meta information associated to each entity. Note Item meta information dataset does not require a timestamp column as target and related time series datasets. Calling Datagroup APIs Once you have the datasets prepared in the described formats, the first step towards building a model on top is to declare those datasets by calling the datagroup APIs. Upload the datasets Before calling the APIs, it's better to make datasets available as remote urls. To do so, there are multiple options: Upload to an Object Storage such as S3, Google Cloud Storage and make the file available to be read (by either making it public or pre-signing it) Upload to a share drive such as Dropbox, Google Drive and generate a sharing url Self host it at a file server (e.g, samba) Declare the datasets using API Step1 . Create a datagroup datagroups serves as the container to host multiple datasets that are related. You can give it whatever name that makes sense to you. python #!/bin/env python import requests import os.path import pandas as pd BASE_ENDPOINT = \"https://forecast.convect.ai/api/\" DEMO_KEY = \"YOUR_API_KEY\" HEADER = { \"Accept\" : \"application/json\" , \"Content-type\" : \"application/json\" , \"Authorization\" : \"Bearer: {} \" . format ( DEMO_KEY ), } # create the data group payload = { \"name\" : \"Demo data group\" } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"data_groups/\" ), json = payload , ) data_group_id = resp . json ()[ \"id\" ] curl #!/bin/env bash export BASE_URL = 'https://forecast.convect.ai/api/' export TOKEN = 'DEMO KEY' export AUTH_HEADER = 'Authorization: Bearer ${TOKEN}' # create a dataset group data_group_id = $( curl --request POST $BASE_URL /data_groups/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"name\": \"Demo data group\"}' | jq '.id' ) Step2 . Declare target time series datasets under a datagroup The next step is to associate individual dataset object to a datagroup . When declaring the dataset , users also need to provide the schema about the dataset, i.e., names of the columns and their roles. For example, below code associate a target time series dataset to a datagroup . python data_url = \"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\" payload = { \"name\" : \"target time series\" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : data_url , \"file_format\" : \"csv\" , \"frequency\" : \"W\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"sku\" , \"col_type\" : \"key\" }, { \"name\" : \"week\" , \"col_type\" : \"time\" }, { \"name\" : \"qty\" , \"col_type\" : \"num\" }, ], } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"datasets/\" ), json = payload , ) dataset_id = resp . json ()[ \"id\" ] curl curl -- request POST $ BASE_URL / datasets / \\ - H 'Content-Type: application/json' \\ - H $ AUTH_HEADER \\ -- data - binary @- << EOF { \"name\" : \"target time series\" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : data_url , \"file_format\" : \"csv\" , \"frequency\" : \"W\" , \"data_group\" : $ { data_group_id }, \"schemas\" : [ { \"name\" : \"sku\" , \"col_type\" : \"key\" }, { \"name\" : \"week\" , \"col_type\" : \"time\" }, { \"name\" : \"qty\" , \"col_type\" : \"num\" }, ], } EOF dataset_type specifies the type of the dataset. Available options are TARGET_TIME_SERIES , RELATED_TIME_SERIES , and ITEM_META . path points to the remote url of the file uploaded. Here we use S3 as the file storage. frequency specifies the frequency the time series was recorded. schemas is a list of dict. Each entry contains name of the column, col_type specifies the type and role of the column. Step3 . (Optional) Declare more dataset types under the data group. Similarly, you can declare more individual datasets, such as related time series and item meta information. Below is an example of declaring an item meta information url = os . path . join ( endpoint , \"datasets/\" ) payload = { \"name\" : f \"item_meta_ { store_id } \" , \"dataset_type\" : \"ITEM_METADATA\" , \"path\" : get_meta_path (), \"file_format\" : \"csv\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"id\" , \"col_type\" : \"key\" }, { \"name\" : \"dept_id\" , \"col_type\" : \"str\" }, { \"name\" : \"cat_id\" , \"col_type\" : \"str\" }, ], } resp = requests . post ( url , json = payload ) resp_payload = resp . json () print ( resp_payload ) return resp_payload [ \"id\" ]","title":"Prepare the data"},{"location":"forecast/data-prepare/#dataset-types-supported","text":"Convect Forecast supports the following types of datasets: Target time series Related time series Item meta information Target time series data is the only required dataset type need to be provided to build a forecasting model. It records the time series values for an entity at different history timestamps. For example, in a retailing setting, it captures the unit sold for each product at different history dates. Related time series data captures time varying values that are not direct targets to be forecasted associated with entities. For example, it may record the price of a product on each date. Item meta information provides additional information that is static across time about the entity to be forecasted. For example, it may contain categorical information such as brands, categories and vendors, about entities.","title":"Dataset types supported"},{"location":"forecast/data-prepare/#data-formats","text":"All 3 types of datasets require one or multiple columns that indicate the identifiers about entities. For example, in the retail setting, these columns may include a SKU plus a store id that uniquely define the identity of an entity to be forecasted.","title":"Data formats"},{"location":"forecast/data-prepare/#target-time-series","text":"The dataset requires at least 3 columns: 1 or more columns serve as the key column(s) mentioned above 1 timestamp column marking when the observation of the target time series value was recorded 1 value column storing the actual value of the time series For example model week qty 1163704 2017-01-01 267 1163704 2017-01-08 229 5998369 2017-01-01 1689 5998369 2017-01-08 1322 where model is the key column of the entity; week is the timestamp column; qty stores the actual time series value.","title":"Target time series"},{"location":"forecast/data-prepare/#related-time-series","text":"Similarly to target time series data, the dataset requires at least 3 columns: 1 or more columns serve as the key column(s) mentioned above 1 timestamp column marking when the observation of the target time series value was recorded 1 or more value columns storing the actual values of the related time series values For example model week price temperature 1163704 2017-01-01 12.5 20.5 1163704 2017-01-08 12.3 22.3 5998369 2017-01-01 5.6 20.5 5998369 2017-01-08 4.5 22.3 where model is the key column of the entity; week is the timestamp column; price and temperature store two related time series values about the entity.","title":"Related time series"},{"location":"forecast/data-prepare/#item-meta-information","text":"Item meta information requires at least 2 columns: 1 or more columns serve as the key column(s) mentioned above 1 or more value columns storing some meta information about the entity For example model brand category 1163704 XXX Food 5998369 YYY Pet where model is the key column of the entity; brand and category are the meta information associated to each entity. Note Item meta information dataset does not require a timestamp column as target and related time series datasets.","title":"Item meta information"},{"location":"forecast/data-prepare/#calling-datagroup-apis","text":"Once you have the datasets prepared in the described formats, the first step towards building a model on top is to declare those datasets by calling the datagroup APIs.","title":"Calling Datagroup APIs"},{"location":"forecast/data-prepare/#upload-the-datasets","text":"Before calling the APIs, it's better to make datasets available as remote urls. To do so, there are multiple options: Upload to an Object Storage such as S3, Google Cloud Storage and make the file available to be read (by either making it public or pre-signing it) Upload to a share drive such as Dropbox, Google Drive and generate a sharing url Self host it at a file server (e.g, samba)","title":"Upload the datasets"},{"location":"forecast/data-prepare/#declare-the-datasets-using-api","text":"Step1 . Create a datagroup datagroups serves as the container to host multiple datasets that are related. You can give it whatever name that makes sense to you. python #!/bin/env python import requests import os.path import pandas as pd BASE_ENDPOINT = \"https://forecast.convect.ai/api/\" DEMO_KEY = \"YOUR_API_KEY\" HEADER = { \"Accept\" : \"application/json\" , \"Content-type\" : \"application/json\" , \"Authorization\" : \"Bearer: {} \" . format ( DEMO_KEY ), } # create the data group payload = { \"name\" : \"Demo data group\" } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"data_groups/\" ), json = payload , ) data_group_id = resp . json ()[ \"id\" ] curl #!/bin/env bash export BASE_URL = 'https://forecast.convect.ai/api/' export TOKEN = 'DEMO KEY' export AUTH_HEADER = 'Authorization: Bearer ${TOKEN}' # create a dataset group data_group_id = $( curl --request POST $BASE_URL /data_groups/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"name\": \"Demo data group\"}' | jq '.id' ) Step2 . Declare target time series datasets under a datagroup The next step is to associate individual dataset object to a datagroup . When declaring the dataset , users also need to provide the schema about the dataset, i.e., names of the columns and their roles. For example, below code associate a target time series dataset to a datagroup . python data_url = \"https://convect-test-data.s3.us-west-2.amazonaws.com/forecast_test_data/target_ts.csv\" payload = { \"name\" : \"target time series\" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : data_url , \"file_format\" : \"csv\" , \"frequency\" : \"W\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"sku\" , \"col_type\" : \"key\" }, { \"name\" : \"week\" , \"col_type\" : \"time\" }, { \"name\" : \"qty\" , \"col_type\" : \"num\" }, ], } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"datasets/\" ), json = payload , ) dataset_id = resp . json ()[ \"id\" ] curl curl -- request POST $ BASE_URL / datasets / \\ - H 'Content-Type: application/json' \\ - H $ AUTH_HEADER \\ -- data - binary @- << EOF { \"name\" : \"target time series\" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : data_url , \"file_format\" : \"csv\" , \"frequency\" : \"W\" , \"data_group\" : $ { data_group_id }, \"schemas\" : [ { \"name\" : \"sku\" , \"col_type\" : \"key\" }, { \"name\" : \"week\" , \"col_type\" : \"time\" }, { \"name\" : \"qty\" , \"col_type\" : \"num\" }, ], } EOF dataset_type specifies the type of the dataset. Available options are TARGET_TIME_SERIES , RELATED_TIME_SERIES , and ITEM_META . path points to the remote url of the file uploaded. Here we use S3 as the file storage. frequency specifies the frequency the time series was recorded. schemas is a list of dict. Each entry contains name of the column, col_type specifies the type and role of the column. Step3 . (Optional) Declare more dataset types under the data group. Similarly, you can declare more individual datasets, such as related time series and item meta information. Below is an example of declaring an item meta information url = os . path . join ( endpoint , \"datasets/\" ) payload = { \"name\" : f \"item_meta_ { store_id } \" , \"dataset_type\" : \"ITEM_METADATA\" , \"path\" : get_meta_path (), \"file_format\" : \"csv\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"id\" , \"col_type\" : \"key\" }, { \"name\" : \"dept_id\" , \"col_type\" : \"str\" }, { \"name\" : \"cat_id\" , \"col_type\" : \"str\" }, ], } resp = requests . post ( url , json = payload ) resp_payload = resp . json () print ( resp_payload ) return resp_payload [ \"id\" ]","title":"Declare the datasets using API"},{"location":"forecast/overview/","text":"Background Convect Forecast provides the tool to automate building and tuning of a forecast model. Forecasting serves as the prerequisite of many decision tasks. For example, when deciding the right inventory level for a product, it's critical to have some insights on the number of units it will be sold in the future, and the uncertainty about the prediction (which is commonly neglected when people are talking about building predictive models). Another example is the need to know how soon a vendor can deliver a batch of orders to your warehouse locations (lead time), when planning for the inventory level. Longer lead times means more uncertainty and higher safety stock levels. Therefore, having a way to quickly build a forecast model is critical when developing such decision applications, so that you can move to building the prescriptive modeling faster. Features Compared to traditional time series modeling, Convect Forecast excels at handling the following problems: Multivariate time series Multiple time series Multivariate time series forecasting problems tackle the situation when there are other features besides the time series itself can provide signals in predicting the future values. This is quite common in e.g., retailing settings, where information about the SKU being predicted, such as brand, category, can help to decide the future demands. Traditional methods that are widely used in ERP systems (such as SAP) and spreadsheet-style forecasting workflows, like Exponential Smoothing , and ARIMA are known to be lacking in providing this feature. Multiple time series forecasting problems tackles the situation when there are many correlated time series need to forecasted with one shot. This is also common in e.g., the retailing setting. Product demands correlate with each other, e.g., in the famous bear and diaper case, which can lead to higher accuracy if such information is utilized when building the model. Popular forecasting library such as Facebook Prophet is known to lacking support for this feature. What does Convect Forecast do for you Convect Forecast ease the process of building forecasting models for multivariate, multiple time series problems. Specifically, it does the following things for users: Preprocess data to make it ready for model building Generate features to be used by models Augment data to increase model accuracy Choose the right strategy and level to build models on Choose the strategy to evaluate model candidates against history data Select the best model across a large pool of candidate models Package the data processing and modeling process as portable bundle so it can be deployed to make predictions on unseen data Workflow To build a model using Convect Forecast, we need to go through the following steps: Prepare data according to the given format Gain insights about the model performance on history data (Optional) Build and generate forecast","title":"Overview"},{"location":"forecast/overview/#background","text":"Convect Forecast provides the tool to automate building and tuning of a forecast model. Forecasting serves as the prerequisite of many decision tasks. For example, when deciding the right inventory level for a product, it's critical to have some insights on the number of units it will be sold in the future, and the uncertainty about the prediction (which is commonly neglected when people are talking about building predictive models). Another example is the need to know how soon a vendor can deliver a batch of orders to your warehouse locations (lead time), when planning for the inventory level. Longer lead times means more uncertainty and higher safety stock levels. Therefore, having a way to quickly build a forecast model is critical when developing such decision applications, so that you can move to building the prescriptive modeling faster.","title":"Background"},{"location":"forecast/overview/#features","text":"Compared to traditional time series modeling, Convect Forecast excels at handling the following problems: Multivariate time series Multiple time series Multivariate time series forecasting problems tackle the situation when there are other features besides the time series itself can provide signals in predicting the future values. This is quite common in e.g., retailing settings, where information about the SKU being predicted, such as brand, category, can help to decide the future demands. Traditional methods that are widely used in ERP systems (such as SAP) and spreadsheet-style forecasting workflows, like Exponential Smoothing , and ARIMA are known to be lacking in providing this feature. Multiple time series forecasting problems tackles the situation when there are many correlated time series need to forecasted with one shot. This is also common in e.g., the retailing setting. Product demands correlate with each other, e.g., in the famous bear and diaper case, which can lead to higher accuracy if such information is utilized when building the model. Popular forecasting library such as Facebook Prophet is known to lacking support for this feature.","title":"Features"},{"location":"forecast/overview/#what-does-convect-forecast-do-for-you","text":"Convect Forecast ease the process of building forecasting models for multivariate, multiple time series problems. Specifically, it does the following things for users: Preprocess data to make it ready for model building Generate features to be used by models Augment data to increase model accuracy Choose the right strategy and level to build models on Choose the strategy to evaluate model candidates against history data Select the best model across a large pool of candidate models Package the data processing and modeling process as portable bundle so it can be deployed to make predictions on unseen data","title":"What does Convect Forecast do for you"},{"location":"forecast/overview/#workflow","text":"To build a model using Convect Forecast, we need to go through the following steps: Prepare data according to the given format Gain insights about the model performance on history data (Optional) Build and generate forecast","title":"Workflow"},{"location":"forecast/run-backtest/","text":"What is backtesting Backtesting is a technique to go back to the past and generate a forecast result at a cutoff date, then compare the result with the actual observations to compute the accuracy metrics of a model. For example, if I would like to generate the sales forecast for a store during the coming Thanksgiving season, a good backtesting strategy is to go back 1 year, to a date prior to Thanksgiving, and generate a forecast result. Then compare it with last year's Thanksgiving sales to gain an understanding how my model is likely to perform for this year, based on last year's performance. Set up a backtesting Before setting up a backtesting experiment, we will go through the same process of preparing data and constructing a predictor-config . Once you have the predictor-config , we first create a backtest-config by import requests import os.path BASE_ENDPOINT = \"https://forecast.convect.ai/api/\" DEMO_KEY = \"YOUR_API_KEY\" HEADER = { \"Accept\" : \"application/json\" , \"Content-type\" : \"application/json\" , \"Authorization\" : \"Bearer: {} \" . format ( DEMO_KEY ), } payload = { backtest_dates : [ \"2019-11-10\" , \"2019-12-20\" ], name : \"holiday sales backtest\" , result_uri : \"s3://convect-data/result/backtest-results/\" , predictor : YOUR_PREDICTOR_ID } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"backtest-configs/\" ), json = payload , ) config_id = resp . json ()[ \"id] Then we trigger the actual runs based on the config by payload = { \"backtest_config\" : config_id } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"backtesters/\" ), json = payload , ) job_id = resp . json ()[ \"id] And you can query the status of the run similarly to the normal forecast runs by resp = requests . get ( os . path . join ( BASE_ENDPOINT , \"backtesters\" , f \" { job_id } /\" ), ) print ( resp . json ())","title":"Evaluating the model"},{"location":"forecast/run-backtest/#what-is-backtesting","text":"Backtesting is a technique to go back to the past and generate a forecast result at a cutoff date, then compare the result with the actual observations to compute the accuracy metrics of a model. For example, if I would like to generate the sales forecast for a store during the coming Thanksgiving season, a good backtesting strategy is to go back 1 year, to a date prior to Thanksgiving, and generate a forecast result. Then compare it with last year's Thanksgiving sales to gain an understanding how my model is likely to perform for this year, based on last year's performance.","title":"What is backtesting"},{"location":"forecast/run-backtest/#set-up-a-backtesting","text":"Before setting up a backtesting experiment, we will go through the same process of preparing data and constructing a predictor-config . Once you have the predictor-config , we first create a backtest-config by import requests import os.path BASE_ENDPOINT = \"https://forecast.convect.ai/api/\" DEMO_KEY = \"YOUR_API_KEY\" HEADER = { \"Accept\" : \"application/json\" , \"Content-type\" : \"application/json\" , \"Authorization\" : \"Bearer: {} \" . format ( DEMO_KEY ), } payload = { backtest_dates : [ \"2019-11-10\" , \"2019-12-20\" ], name : \"holiday sales backtest\" , result_uri : \"s3://convect-data/result/backtest-results/\" , predictor : YOUR_PREDICTOR_ID } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"backtest-configs/\" ), json = payload , ) config_id = resp . json ()[ \"id] Then we trigger the actual runs based on the config by payload = { \"backtest_config\" : config_id } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"backtesters/\" ), json = payload , ) job_id = resp . json ()[ \"id] And you can query the status of the run similarly to the normal forecast runs by resp = requests . get ( os . path . join ( BASE_ENDPOINT , \"backtesters\" , f \" { job_id } /\" ), ) print ( resp . json ())","title":"Set up a backtesting"},{"location":"forecast/run-forecast/","text":"Forecast configuration Once you finish declaring all the datasets associated with a datagroup , you can proceed to set up a forecast configuration. The configuration defines the forecast task, e.g., how long should the forecast horizon be, what metrics to use when evaluating models, if uncertainty interval should be included in the results, where to write the result files. For example, below code sets up a task that outputs the consecutive 14-week predicted value starting from the last date in the dataset. python url = os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ) output_path = \"s3://convect-data/result/demo-run\" payload = { \"name\" : \"14 week forecast config\" , \"result_uri\" : output_path , \"horizon\" : 14 , \"frequency\" : \"W\" , \"data_group\" : data_group_id , } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ), json = payload , ) config_id = resp . json ()[ \"id\" ] curl output_path = 's3://convect-data/result/demo-run' config_id = $( curl --request POST $BASE_URL /predictor-configs/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data-binary @- << EOF { \"name\": \"12 week forecast config\", \"result_uri\": output_path, \"horizon\": 14, \"frequency\": \"W\", \"data_group\": data_group_id, } EOF | jq '.id' ) Trigger the forecast Once a config is set up, we can trigger the actual forecast run by calling POST endpoint predictors/ by providing the id of the config. python payload = { \"predictor_config\" : config_id } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictors/\" ), json = payload ) predictor_id = resp . json ()[ \"id\" ] curl run_id = $( curl --request POST $BASE_URL /predictors/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"predictor_config\": \"${config_id}\"}' | jq '.id' ) Retrieve the result Once a run is triggered, query the status of the run by calling GET endpoint predictors/{id} . python resp = requests . get ( os . path . join ( BASE_ENDPOINT , \"predictors/\" , f \" { predictor_id } /\" ) ) print ( resp . json ()) curl curl --request GET $BASE_URL /predictors/ ${ run_id } / \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER Once the returned status is indicated as Succeeded , you can read the result from the output path specified when setting up the forecast config. Below is an example of the result file key sku predict_sum predict_start_date predict_horizon 11000015 11000015 1.000000027104373 2021-08-31 7 11000016 11000016 3.000000027104373 2021-08-31 7 11000017 11000017 1.000000027104373 2021-08-31 7 11000018 11000018 1.0577409169104601 2021-08-31 7 11000019 11000019 0.998951528482072 2021-08-31 7 11000020 11000020 1.0051190404354284 2021-08-31 7 11000021 11000021 0.9999988959151295 2021-08-31 7","title":"Running the forecast"},{"location":"forecast/run-forecast/#forecast-configuration","text":"Once you finish declaring all the datasets associated with a datagroup , you can proceed to set up a forecast configuration. The configuration defines the forecast task, e.g., how long should the forecast horizon be, what metrics to use when evaluating models, if uncertainty interval should be included in the results, where to write the result files. For example, below code sets up a task that outputs the consecutive 14-week predicted value starting from the last date in the dataset. python url = os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ) output_path = \"s3://convect-data/result/demo-run\" payload = { \"name\" : \"14 week forecast config\" , \"result_uri\" : output_path , \"horizon\" : 14 , \"frequency\" : \"W\" , \"data_group\" : data_group_id , } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictor-configs/\" ), json = payload , ) config_id = resp . json ()[ \"id\" ] curl output_path = 's3://convect-data/result/demo-run' config_id = $( curl --request POST $BASE_URL /predictor-configs/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data-binary @- << EOF { \"name\": \"12 week forecast config\", \"result_uri\": output_path, \"horizon\": 14, \"frequency\": \"W\", \"data_group\": data_group_id, } EOF | jq '.id' )","title":"Forecast configuration"},{"location":"forecast/run-forecast/#trigger-the-forecast","text":"Once a config is set up, we can trigger the actual forecast run by calling POST endpoint predictors/ by providing the id of the config. python payload = { \"predictor_config\" : config_id } resp = requests . post ( os . path . join ( BASE_ENDPOINT , \"predictors/\" ), json = payload ) predictor_id = resp . json ()[ \"id\" ] curl run_id = $( curl --request POST $BASE_URL /predictors/ \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER \\ --data '{\"predictor_config\": \"${config_id}\"}' | jq '.id' )","title":"Trigger the forecast"},{"location":"forecast/run-forecast/#retrieve-the-result","text":"Once a run is triggered, query the status of the run by calling GET endpoint predictors/{id} . python resp = requests . get ( os . path . join ( BASE_ENDPOINT , \"predictors/\" , f \" { predictor_id } /\" ) ) print ( resp . json ()) curl curl --request GET $BASE_URL /predictors/ ${ run_id } / \\ -H 'Content-Type: application/json' \\ -H $AUTH_HEADER Once the returned status is indicated as Succeeded , you can read the result from the output path specified when setting up the forecast config. Below is an example of the result file key sku predict_sum predict_start_date predict_horizon 11000015 11000015 1.000000027104373 2021-08-31 7 11000016 11000016 3.000000027104373 2021-08-31 7 11000017 11000017 1.000000027104373 2021-08-31 7 11000018 11000018 1.0577409169104601 2021-08-31 7 11000019 11000019 0.998951528482072 2021-08-31 7 11000020 11000020 1.0051190404354284 2021-08-31 7 11000021 11000021 0.9999988959151295 2021-08-31 7","title":"Retrieve the result"},{"location":"tutorials/custom-pipeline/","text":"WIP","title":"Custom forecasting pipeline"},{"location":"tutorials/m5/","text":"Background This tutorial walks through how to utilize Convect Automated Forecasting API to finish the M5 Forecasting challenge. Prepare the data Because the M5 data is in the wide format , we first need to convert them to the regular format as specified in data preparation section . import pandas as pd # read the data df_sales = pd . read_csv ( \"data/sales_train_evaluation.csv\" ) df_cal = pd . read_csv ( \"data/calendar.csv\" ) store_id_set = df_sales . store_id . unique () idcols = [ \"id\" , \"item_id\" , \"dept_id\" , \"cat_id\" , \"store_id\" , \"state_id\" , ] # function to split data by store # and convert wide format to long format def process_store ( store_id , ): print ( store_id , \"START\" ) df_store = df_sales [ df_sales . store_id == store_id ] df_ts = pd . melt ( df_store , id_vars = idcols , value_name = \"sales\" ) df_ts = df_ts . merge ( df_cal [[ \"date\" , \"d\" ]], left_on = \"variable\" , right_on = \"d\" , how = \"left\" , ) df_ts [[ \"id\" , \"date\" , \"sales\" ]] . to_csv ( f \"data/by_store/ { store_id . lower () } _target_time_series.csv.gz\" , index = False , compression = \"gzip\" , ) print ( store_id , \"END\" ) # trigger the processing from multiprocessing import Pool from itertools import product with Pool ( processes = 6 ) as p : p . starmap ( process_store , product ( store_id_set )) # process the meta data df_meta = ( df_sales [ idcols ] . drop_duplicates () . drop ([ \"store_id\" , \"state_id\" , \"item_id\" ], axis = 1 ) ) df_meta . to_csv ( f \"data/by_store/m5_item_meta.csv\" , index = False ) Run the forecasts Once we have the data split by store, we can build a model for each store level dataset. import pandas as pd import requests import os.path \"\"\" Upload files to s3 !aws s3 sync data/by_store/ s3://convect-data/m5/by_store/ \"\"\" stores = [ \"ca_1\" , \"ca_2\" , \"ca_3\" , \"ca_4\" , \"tx_1\" , \"tx_2\" , \"tx_3\" , \"wi_1\" , \"wi_2\" , \"wi_3\" , ] endpoint = \"https://forecast.convect.ai/api/\" # helper functions def get_ts_path ( store_id ): return f \"s3://convect-data/m5/by_store/ { store_id . lower () } _target_time_series.csv.gz\" def get_meta_path (): return f \"s3://convect-data/m5/by_store/m5_item_meta.csv\" def create_data_group ( store_id ): url = os . path . join ( endpoint , \"data-groups/\" ) payload = { \"name\" : f \" { store_id } -data-groups\" } url = os . path . join ( endpoint , \"data-groups/\" ) resp = requests . post ( url , json = payload ) resp_payload = resp . json () return resp_payload [ \"id\" ] def create_ts_dataset ( store_id , data_group_id ): url = os . path . join ( endpoint , \"datasets/\" ) payload = { \"name\" : f \"target_time_series_ { store_id } \" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : get_ts_path ( store_id ), \"file_format\" : \"csv\" , \"frequency\" : \"D\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"id\" , \"col_type\" : \"key\" }, { \"name\" : \"date\" , \"col_type\" : \"time\" }, { \"name\" : \"sales\" , \"col_type\" : \"num\" }, ], } resp = requests . post ( url , json = payload ) resp_payload = resp . json () print ( resp_payload ) return resp_payload [ \"id\" ] def create_meta_dataset ( store_id , data_group_id ): url = os . path . join ( endpoint , \"datasets/\" ) payload = { \"name\" : f \"item_meta_ { store_id } \" , \"dataset_type\" : \"ITEM_METADATA\" , \"path\" : get_meta_path (), \"file_format\" : \"csv\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"id\" , \"col_type\" : \"key\" }, { \"name\" : \"dept_id\" , \"col_type\" : \"str\" }, { \"name\" : \"cat_id\" , \"col_type\" : \"str\" }, ], } resp = requests . post ( url , json = payload ) resp_payload = resp . json () print ( resp_payload ) return resp_payload [ \"id\" ] def create_predictor_config ( store_id , data_group_id ): url = os . path . join ( endpoint , \"predictor-configs/\" ) payload = { \"name\" : f \"validation-forecast-config- { store_id } \" , \"result_uri\" : f \"s3://convect-data/test-data/m5-api-result/ { store_id } /\" , \"horizon\" : 28 , \"frequency\" : \"D\" , \"data_group\" : data_group_id , } res = requests . post ( url , json = payload ) resp_payload = res . json () print ( resp_payload ) return resp_payload [ \"id\" ] def create_predictor ( config_id ): url = os . path . join ( endpoint , \"predictors/\" ) payload = { \"predictor_config\" : config_id } res = requests . post ( url , json = payload ) resp_payload = res . json () print ( resp_payload ) return resp_payload [ \"id\" ] def query_predictor_status ( predictor_id ): url = os . path . join ( endpoint , \"predictors/\" ) res = requests . get ( os . path . join ( url , str ( predictor_id ))) return res . json () def start_prediction_by_api ( store_id ): data_group_id = create_data_group ( store_id ) _ = create_ts_dataset ( store_id , data_group_id ) _ = create_meta_dataset ( store_id , data_group_id ) config_id = create_predictor_config ( store_id , data_group_id ) predictor_id = create_predictor ( config_id ) return query_predictor_status ( predictor_id ) # trigger the prediction for store in stores : start_prediction_by_api ( store ) Combine the results Once all the models finish, we can read and combine all the store level results, post-process them in the format ready for submission to Kaggle. import pandas as pd stores = [ \"ca_1\" , \"ca_2\" , \"ca_3\" , \"ca_4\" , \"tx_1\" , \"tx_2\" , \"tx_3\" , \"wi_1\" , \"wi_2\" , \"wi_3\" , ] # combine the resuls base_out_path = \"s3://convect-data/test-data/m5-api-result/by_store/ {} /data\" dfs = [] for store in stores : dfs . append ( pd . read_csv ( base_out_path . format ( store ))) df_pred = pd . concat ( dfs , axis = 0 ) . drop ( [ \"Unnamed: 0\" ], axis = 1 ) df_pred [ \"dt_label\" ] = pd . to_datetime ( df_pred [ \"predict_start_date\" ] ) - pd . to_datetime ( \"2016-04-24\" ) df_pred [ \"dt_label\" ] = \"F\" + df_pred [ \"dt_label\" ] . dt . days . astype ( str ) df_pred [ \"id\" ] = df_pred [ \"id\" ] . map ( lambda x : x + \"_\" + \"evaluation\" ) # long to wide format df_wide = df_pred [[ \"id\" , \"predict_sum\" , \"dt_label\" ]] . pivot ( index = \"id\" , columns = \"dt_label\" , values = \"predict_sum\" ) # reorder the columns cols = list ( map ( lambda x : f \"F { x } \" , range ( 1 , 29 ))) df_wide = df_wide [ cols ] df_wide . to_csv ( \"submission.csv\" )","title":"M5 forecasting"},{"location":"tutorials/m5/#background","text":"This tutorial walks through how to utilize Convect Automated Forecasting API to finish the M5 Forecasting challenge.","title":"Background"},{"location":"tutorials/m5/#prepare-the-data","text":"Because the M5 data is in the wide format , we first need to convert them to the regular format as specified in data preparation section . import pandas as pd # read the data df_sales = pd . read_csv ( \"data/sales_train_evaluation.csv\" ) df_cal = pd . read_csv ( \"data/calendar.csv\" ) store_id_set = df_sales . store_id . unique () idcols = [ \"id\" , \"item_id\" , \"dept_id\" , \"cat_id\" , \"store_id\" , \"state_id\" , ] # function to split data by store # and convert wide format to long format def process_store ( store_id , ): print ( store_id , \"START\" ) df_store = df_sales [ df_sales . store_id == store_id ] df_ts = pd . melt ( df_store , id_vars = idcols , value_name = \"sales\" ) df_ts = df_ts . merge ( df_cal [[ \"date\" , \"d\" ]], left_on = \"variable\" , right_on = \"d\" , how = \"left\" , ) df_ts [[ \"id\" , \"date\" , \"sales\" ]] . to_csv ( f \"data/by_store/ { store_id . lower () } _target_time_series.csv.gz\" , index = False , compression = \"gzip\" , ) print ( store_id , \"END\" ) # trigger the processing from multiprocessing import Pool from itertools import product with Pool ( processes = 6 ) as p : p . starmap ( process_store , product ( store_id_set )) # process the meta data df_meta = ( df_sales [ idcols ] . drop_duplicates () . drop ([ \"store_id\" , \"state_id\" , \"item_id\" ], axis = 1 ) ) df_meta . to_csv ( f \"data/by_store/m5_item_meta.csv\" , index = False )","title":"Prepare the data"},{"location":"tutorials/m5/#run-the-forecasts","text":"Once we have the data split by store, we can build a model for each store level dataset. import pandas as pd import requests import os.path \"\"\" Upload files to s3 !aws s3 sync data/by_store/ s3://convect-data/m5/by_store/ \"\"\" stores = [ \"ca_1\" , \"ca_2\" , \"ca_3\" , \"ca_4\" , \"tx_1\" , \"tx_2\" , \"tx_3\" , \"wi_1\" , \"wi_2\" , \"wi_3\" , ] endpoint = \"https://forecast.convect.ai/api/\" # helper functions def get_ts_path ( store_id ): return f \"s3://convect-data/m5/by_store/ { store_id . lower () } _target_time_series.csv.gz\" def get_meta_path (): return f \"s3://convect-data/m5/by_store/m5_item_meta.csv\" def create_data_group ( store_id ): url = os . path . join ( endpoint , \"data-groups/\" ) payload = { \"name\" : f \" { store_id } -data-groups\" } url = os . path . join ( endpoint , \"data-groups/\" ) resp = requests . post ( url , json = payload ) resp_payload = resp . json () return resp_payload [ \"id\" ] def create_ts_dataset ( store_id , data_group_id ): url = os . path . join ( endpoint , \"datasets/\" ) payload = { \"name\" : f \"target_time_series_ { store_id } \" , \"dataset_type\" : \"TARGET_TIME_SERIES\" , \"path\" : get_ts_path ( store_id ), \"file_format\" : \"csv\" , \"frequency\" : \"D\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"id\" , \"col_type\" : \"key\" }, { \"name\" : \"date\" , \"col_type\" : \"time\" }, { \"name\" : \"sales\" , \"col_type\" : \"num\" }, ], } resp = requests . post ( url , json = payload ) resp_payload = resp . json () print ( resp_payload ) return resp_payload [ \"id\" ] def create_meta_dataset ( store_id , data_group_id ): url = os . path . join ( endpoint , \"datasets/\" ) payload = { \"name\" : f \"item_meta_ { store_id } \" , \"dataset_type\" : \"ITEM_METADATA\" , \"path\" : get_meta_path (), \"file_format\" : \"csv\" , \"data_group\" : data_group_id , \"schemas\" : [ { \"name\" : \"id\" , \"col_type\" : \"key\" }, { \"name\" : \"dept_id\" , \"col_type\" : \"str\" }, { \"name\" : \"cat_id\" , \"col_type\" : \"str\" }, ], } resp = requests . post ( url , json = payload ) resp_payload = resp . json () print ( resp_payload ) return resp_payload [ \"id\" ] def create_predictor_config ( store_id , data_group_id ): url = os . path . join ( endpoint , \"predictor-configs/\" ) payload = { \"name\" : f \"validation-forecast-config- { store_id } \" , \"result_uri\" : f \"s3://convect-data/test-data/m5-api-result/ { store_id } /\" , \"horizon\" : 28 , \"frequency\" : \"D\" , \"data_group\" : data_group_id , } res = requests . post ( url , json = payload ) resp_payload = res . json () print ( resp_payload ) return resp_payload [ \"id\" ] def create_predictor ( config_id ): url = os . path . join ( endpoint , \"predictors/\" ) payload = { \"predictor_config\" : config_id } res = requests . post ( url , json = payload ) resp_payload = res . json () print ( resp_payload ) return resp_payload [ \"id\" ] def query_predictor_status ( predictor_id ): url = os . path . join ( endpoint , \"predictors/\" ) res = requests . get ( os . path . join ( url , str ( predictor_id ))) return res . json () def start_prediction_by_api ( store_id ): data_group_id = create_data_group ( store_id ) _ = create_ts_dataset ( store_id , data_group_id ) _ = create_meta_dataset ( store_id , data_group_id ) config_id = create_predictor_config ( store_id , data_group_id ) predictor_id = create_predictor ( config_id ) return query_predictor_status ( predictor_id ) # trigger the prediction for store in stores : start_prediction_by_api ( store )","title":"Run the forecasts"},{"location":"tutorials/m5/#combine-the-results","text":"Once all the models finish, we can read and combine all the store level results, post-process them in the format ready for submission to Kaggle. import pandas as pd stores = [ \"ca_1\" , \"ca_2\" , \"ca_3\" , \"ca_4\" , \"tx_1\" , \"tx_2\" , \"tx_3\" , \"wi_1\" , \"wi_2\" , \"wi_3\" , ] # combine the resuls base_out_path = \"s3://convect-data/test-data/m5-api-result/by_store/ {} /data\" dfs = [] for store in stores : dfs . append ( pd . read_csv ( base_out_path . format ( store ))) df_pred = pd . concat ( dfs , axis = 0 ) . drop ( [ \"Unnamed: 0\" ], axis = 1 ) df_pred [ \"dt_label\" ] = pd . to_datetime ( df_pred [ \"predict_start_date\" ] ) - pd . to_datetime ( \"2016-04-24\" ) df_pred [ \"dt_label\" ] = \"F\" + df_pred [ \"dt_label\" ] . dt . days . astype ( str ) df_pred [ \"id\" ] = df_pred [ \"id\" ] . map ( lambda x : x + \"_\" + \"evaluation\" ) # long to wide format df_wide = df_pred [[ \"id\" , \"predict_sum\" , \"dt_label\" ]] . pivot ( index = \"id\" , columns = \"dt_label\" , values = \"predict_sum\" ) # reorder the columns cols = list ( map ( lambda x : f \"F { x } \" , range ( 1 , 29 ))) df_wide = df_wide [ cols ] df_wide . to_csv ( \"submission.csv\" )","title":"Combine the results"}]}